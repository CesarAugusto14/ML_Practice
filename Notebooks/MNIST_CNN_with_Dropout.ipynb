{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvrlQGAvLfdM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvGGpQgXWehr"
      },
      "source": [
        "# MNIST Classification problem\n",
        "\n",
        "This is a notebook to perform the MNIST Classification Problem by using a Convolutional Neural Network. The scope of the problem is to classify properly handwritten numbers. We have 60000 numbers on our Training set, 10000 numbers on our test set. \n",
        "\n",
        "We also have 10 classes, whose are the numbers from 0 to 9. \n",
        "\n",
        "## PyTorch Implementation:\n",
        "\n",
        "For achieving this task, we will use this notebook to learn how to use the different implementations of PyTorch. We will need installed PyTorch, some parts of the PyTorch Ecosystem, and torchsummary. Also, we will need to install Scikit-Learn for showing the Confusion Matrix. \n",
        "\n",
        "Our first task is to load the Data. \n",
        "\n",
        "<b>REMEMBER ALWAYS TO HAVE YOUR GPU WORKING</b>\n",
        "\n",
        "### Links to the Documentation:\n",
        "\n",
        "- <a href=\"https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST\">Here</a> the user will be able to find how to use the MNIST object from PyTorch. The user could also download the data directly from <a href=\"http://yann.lecun.com/exdb/mnist/index.html\">Yann LeCun's repository.</a>\n",
        "\n",
        "- For accessing the documentation for the random splitter, the user can refer to <a href=\"https://pytorch.org/docs/stable/data.html#\"> documentation.</a> Note that the documentation is for the data library from PyTorch, which has inside the random splitter. \n",
        "\n",
        "- For accessing to the DataLoader documentation, you can follow   <a href=\"https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\"> this link.</a>\n",
        "\n",
        "### Loading the Data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fmhd1tK2LfdR"
      },
      "outputs": [],
      "source": [
        "image_path = '../data'\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "mnist_dataset = torchvision.datasets.MNIST(\n",
        "    root=image_path,\n",
        "    train=True,\n",
        "    transform=transform,\n",
        "    download=True\n",
        ")\n",
        "mnist_test_dataset = datasets.MNIST(\n",
        "    root=image_path,\n",
        "    train=False,\n",
        "    transform=transform,\n",
        "    download=False\n",
        ")\n",
        "# Train/ Validation Split (I am using the splitter from PyTorch.)\n",
        "mnist_train_dataset, mnist_val_dataset = random_split(mnist_dataset, [50000, 10000])\n",
        "\n",
        "# Call DataLoaders:\n",
        "train_dl = DataLoader(mnist_train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "val_dl = DataLoader(mnist_val_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "# As I'm not using the test set sequentially, I won't put test set in a DataLoader."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD3kDFsra9nc"
      },
      "source": [
        "## Creating a Model with PyTorch:\n",
        "\n",
        "For this problem, I am using the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html\"> nn.Sequential() class</a> to create a model. Also, I am inheriting all the methods from the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Module.html\"> nn.Module() object</a>. \n",
        "\n",
        "Note that I will also train my model by parallelizing computations with a GPU. This is done by calling model().to(device).\n",
        "\n",
        "Finally, note that as an output for the following cell, we can see a summary of the model, similar to the summary provided by Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wf0_NtyRLfdU",
        "outputId": "adcbcef0-19bd-464e-cc87-d09630c49786"
      },
      "outputs": [],
      "source": [
        "# Model creation WITHOUT DROPOUT LAYER!\n",
        "class model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(model, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding='valid'),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding='valid'),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(in_channels= 64, out_channels=128, kernel_size=3, padding='valid'),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=128, out_features=512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.Linear(in_features=512, out_features=128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
        "model = model().to(device)\n",
        "\n",
        "# Summary:\n",
        "summary(model.main, input_size=(1,28, 28), batch_size=128)\n",
        "# Selection of Loss and Optimizer:\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UVdWMuCb_VY"
      },
      "source": [
        "## Training of the Model\n",
        "\n",
        "More than trying to achieve the highest possible score, on this notebook we are aiming to regularize the model by adding dropout layers and performing EarlyStopping during the training of the network. \n",
        "\n",
        "Each part of the training function is commented to aid the easy reading of the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_c388vMLfdZ",
        "outputId": "116422d9-16b9-4328-f7f2-12e708097158"
      },
      "outputs": [],
      "source": [
        "# Training Script for Classification tasks, with EarlyStopping:\n",
        "\n",
        "\n",
        "def train(model, train_dl, num_epochs=100, patience=5):\n",
        "\n",
        "    # Lists for storing the training values:\n",
        "    training_loss = []\n",
        "    training_acc = []\n",
        "    val_loss = []\n",
        "    val_acc = []\n",
        "    # Control for the Early Stopping:\n",
        "    best_acc = 0\n",
        "    counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Initialize the running loss and accuracy for estimating the metrics \n",
        "        # during the running of each batch. \n",
        "        running_loss = 0\n",
        "        running_acc = 0\n",
        "\n",
        "        for x_batch, y_batch in tqdm(train_dl):\n",
        "            # Prediciton:\n",
        "            pred = model(x_batch.to(device))\n",
        "            # Computing the Loss:\n",
        "            loss = loss_fn(pred, y_batch.to(device))\n",
        "            # We want to get an estimate of the loss per epoch\n",
        "            running_loss += loss.item()/len(train_dl)\n",
        "            # Backpropagation of the loss:\n",
        "            loss.backward()\n",
        "            # Actualization of the weights:\n",
        "            optimizer.step()\n",
        "            # Setting to zero the grads.\n",
        "            optimizer.zero_grad()\n",
        "            # Computing running accuracy.\n",
        "            correct = (torch.argmax(pred, dim=1) == y_batch.to(device)).float()\n",
        "            running_acc += correct.sum()/len(train_dl.dataset)\n",
        "        # History of the loss and accuracy per Epoch:\n",
        "        training_loss.append(running_loss)\n",
        "        training_acc.append(running_acc.cpu())\n",
        "        print(\n",
        "            f'Epoch {epoch+1} / {num_epochs}. Training Loss = {running_loss: .4f} '\n",
        "            f'Training_acc = {running_acc : .4f}')\n",
        "\n",
        "        # Doing the same for the validation.\n",
        "        # (Difference: We don't compute the gradients)\n",
        "\n",
        "        running_loss = 0\n",
        "        running_acc = 0\n",
        "        for x_batch, y_batch in val_dl:\n",
        "            pred = model(x_batch.to(device))\n",
        "            loss = loss_fn(pred, y_batch.to(device))\n",
        "            running_loss += loss.item()/len(train_dl)\n",
        "            correct = (torch.argmax(pred, dim=1) == y_batch.to(device)).float()\n",
        "            running_acc += correct.sum()/len(val_dl.dataset)\n",
        "        val_loss.append(running_loss)\n",
        "        val_acc.append(running_acc.cpu())\n",
        "        print(\n",
        "            f'Val_Loss = {running_loss: .4f} '\n",
        "            f'Val_acc = {running_acc : .4f}')\n",
        "        \n",
        "        # Early Stopping Criteria, note that if the accuracy is not improving\n",
        "        # after several epochs (given by the patience), it will stop and save\n",
        "        # the model that is giving best results on unseen data. \n",
        "        \n",
        "        if float(running_acc) < float(best_acc):\n",
        "            counter +=1\n",
        "            if counter > patience:\n",
        "                print('Early stopping achieved.')\n",
        "                model=snapshot\n",
        "                return training_loss, training_acc, val_loss, val_acc\n",
        "        else:\n",
        "            counter = 0\n",
        "            best_acc = running_acc\n",
        "            snapshot = model\n",
        "            \n",
        "        print(f'Best val_acc = {best_acc}')        \n",
        "        print(f'counter is {counter}')\n",
        "        \n",
        "    return training_loss, training_acc, val_loss, val_acc\n",
        "\n",
        "\n",
        "hist = train(model, num_epochs=20, patience=3, train_dl=train_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing the Training of the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "Ejer70UPLfdd",
        "outputId": "2951eb22-90b5-4b86-e4d8-e8f95aff6ee2"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(hist[0], 'r--',lw=2, label='Training Loss')\n",
        "plt.plot(hist[2], 'k',lw=2, label='Validation Loss')\n",
        "plt.legend(loc=1)\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "vujCaubrLfdh",
        "outputId": "652f83f6-f887-4482-d099-71fee000da22"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(hist[1], 'r--',lw=2, label='Training Accuracy')\n",
        "plt.plot(hist[3], 'k',lw=2, label='Validation Accuracy')\n",
        "plt.legend(loc=4)\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Accuracy + Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "UPJ6M1vMLfdk",
        "outputId": "425bcab2-8020-420d-d54d-047abfc6053d"
      },
      "outputs": [],
      "source": [
        "model.to('cpu')\n",
        "pred = torch.argmax(model((mnist_test_dataset.data.unsqueeze(1)/255)), dim=1)\n",
        "\n",
        "correct = (pred == mnist_test_dataset.targets).float()\n",
        "test_acc = correct.sum()/len(mnist_test_dataset.targets)\n",
        "conf_matrix_1 = confusion_matrix(mnist_test_dataset.targets, pred)\n",
        "fig, ax = plt.subplots(figsize=(9, 9))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_1,)\n",
        "disp.plot(ax=ax, cmap=plt.cm.magma, colorbar=False)\n",
        "plt.title(f'The Test Accuracy is : {test_acc*100 : .4f} %')\n",
        "plt.savefig('conf_MNIST_cnn_no_dropout.jpg', bbox_inches='tight')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "MNIST_CNN_with_Dropout.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
